\chapter{System call based detection}
\label{chap:syscalltheory}
In this chapter, the overall strategy of our second approach for malware detection is discussed, which uses system call traces of applications to predict malicious activity. The next chapter (chapter~\ref{chap:syscallexperiment}) discusses the implementation details of this model. The results achieved in the experiment using this model is detailed and explained in chapter~\ref{chap:results}.\\

The central idea is to run an application for a specific amount of time. During its execution, the details about the system calls it makes to the operating system are recorded. We developed a machine learning model/classifier that can detect malware based on its system call trace. The syscall records of known malwares and known non-malwares are used to train the classifier. There are two phases: training and classification. How the known malware and non-malware traces will be used to train the classifier is described in section ~\ref{sec:syscalltheorytrain}. Section~\ref{sec:syscalltheoryclassify} decribes how the model will classify an unknown app using its syscall trace.\\

\section{Training}
\label{sec:syscalltheorytrain}
We used a set of applications consisting both known malwares and known non-malwares as the training dataset. We collected system call traces of all applications of the training dataset (all of the applications are run for a specific amount of time). The system call trace of a single application is a list of system calls the application used during execution. For example: \textbf{\{recv, semget, msgget, \ldots \}}; where \textbf{recv}, \textbf{semget}, \textbf{msgget} are system calls.\\

After collecting system call traces, We aggregate this traces to create two binary relation matrices $M_{mal}$ and $M_{nmal}$. $M_{mal}$ shows relation between system calls and malware applications, Where $M_{nmal}$ shows relationship between system calls and non-malwares. $M_{mal}$ and $M_{nmal}$ matrices are defined as follows:\\
\[ M_{mal}(i,j) =
  \begin{cases}
      1       & \quad \text{if } i^{th} \text{ malware uses } j^{th} \text{ syscall}\\
      0       & \quad \text{otherwise}\\
  \end{cases}
\]\\
\[ M_{nmal}(i,j) =
  \begin{cases}
      1       & \quad \text{if } i^{th} \text{ non-malware uses } j^{th} \text{ syscall}\\
      0       & \quad \text{otherwise}\\
  \end{cases}
\]
\\\\
Then we calculate the Goodness Rating of $j^{th}$ syscall, $\gamma_j$ as follows,
\[
\gamma_j = \frac{1}{N_{nmal}}\sum_{i=1}^{N_{nmal}} M_{nmal}(i,j) - \frac{1}{N_{mal}}\sum_{i=1}^{N_{mal}} M_{mal}(i,j)
\]
where $N_{nmal}$ and $N_{mal}$ are number of non-malware and malware samples.
\\
\section{Classification}
\label{sec:syscalltheoryclassify}
To classify an unknown application as \emph{malware} or \emph{non-malware}, first, we execute the application for the same time duration duration we used with each training application. We collect the system call trace of that application during that execution, same as before. But this time, we also record the frequency of each syscall used by the application during execution. So now, the syscall trace of an application during classification phase can be expressed as a list of pairs of syscalls and their frequencies. For example: \textbf{\{(recv,1032), (semget, 143), \ldots \}} is a trace of an application which called the \textbf{recv} routine 1032 times, \textbf{semget} 143 times and so on.

Then we define the Goodness Rating of that application, $\gamma_{app}$ as follows,
\[
\gamma_{app} = \sum_{s \in S_{app}} \gamma_s \times f_s
\]

Where, $S_{app}$ is the set of system calls used by $app$, $\gamma_s$ is the Goodness rating of syscall $s$ and $f_s$ is the frequency of syscall $s$ in $app$.

If we assume that malwares uses similar system calls which are distinctive from those used by non-malwares; It is logical to assume that a malware will use more syscalls those has lower goodness ratings and less syscalls having higher goodness ratings. The opposite can be said for non-malware applications. So this will result in higher goodness ratings of non-malware applications and lower goodness rating for malware applications.

Now for classification, we check if the goodness rating of the application under inspection exceeds some threshold. If so, we classify it as non-malware. Otherwise we flag it as malware. 
\[
\begin{cases}
      \text{app is a malware}  & \quad \text{if } \gamma_{app} > \tau\\
      \text{app is not a malware} & \quad \text{otherwise}\\
  \end{cases}
\]

Where, $\tau$ is a threshold. Theoretically, the threshold should be zero. But it actually depends on the experiment and the training data used. We used this approach to classify apps in our validation dataset and calculated \textbf{accuracy} ($\alpha$), \textbf{recall} ($\rho$), \textbf{specificity} ($\sigma$), \textbf{precision} ($\phi$) and \textbf{f-measure} ($F$). These metrics are defined in section~\ref{sec:netanalysisresults}.


























\chapter{Experiment on System call based classifier}
\label{chap:syscallexperiment}
In this chapter, we went into details on the experiment we conducted to validate our model. 

\section{Preparing experiment}
\label{sec:syscallexpprepare}
We collected system call traces of all applications using a single device. The reason behind this is we intended to provide identical environments for all applications to execute in. The device was reset to factory default configuration and the device needed to be \emph{rooted}. We used standard linux utility \emph{strace} to trace system call of applications. We also used \emph{timeout} command to run every application for a fixed duration of time. Although \emph{strace} and \emph{timeout} are standard linux utilities, they are not included in standard Android builds. So we had to collect the source code of this tools and cross-compile them for the CPU architecture of the device on which the experiment was run. The compilation task required \emph{Android NDK}. After the binaries are created for our desired CPU architecture (in our case \textbf{ARMv7}), they are put in the \textbf{/system/xbin} of our device, so that they can be accessed by a shell script run through \emph{ADB}. Copying any binary into \textbf{/system} requires superuser permission, that is one of the reasons why we needed to \emph{root} our device at the first place. Additionally, necessary drivers and Android SDK are required on the host machine, where the script would run.

\section{Experiment}
\label{sec:syscallexpsim}
We planned to collect system call traces of a total of 453 malwares and 227 non-malwares. The number of apps used in validation and training is shown in table~\ref{table:numofapps}. 

\begin{table}[h]
    \centering
    \bgroup
    \def\arraystretch{1.9}%
    \begin{tabular}{ |c|c|c|c|} 
    \hline
    & Training & Validation & Total\\ 
    \hline
    Malware & 403 & 50 & 453\\  
    \hline
    Non-malware & 177 & 50 & 227\\  
    \hline
    \end{tabular}
    \caption{Number of apps used in the experiment}
    \label{table:numofapps}
    \egroup
\end{table}


We wrote a batch script that automates the whole process. The script executes commands in the device using \emph{ADB}. The workflow of the script is outlined in Algorithm~\ref{algo:mainsimulation}.

\begin{algorithm}
\caption{Syscall trace collect script}\label{algo:mainsimulation}
\begin{algorithmic}[1]
\Procedure{Collect-All-Syscall-Trace}{$directory$}
\For{each apk file in $directory$}
\State $pckgname \gets$ get package name from that apk using \textbf{aapt}
\State Install the apk in the device.
\State Launch the app
\State $pid \gets$ ps($pckgname$)
\State $stracelogs[pckgname] \gets$ output of \textbf{strace($pid$)} with 20 seconds timeout 
\State Force close the app
\State Uninstall the app
\EndFor
\State \textbf{return} $stracelogs$
\EndProcedure
\end{algorithmic}
\end{algorithm}

The exact script is given in Appendix B (See Listing B.4).\\

We have two directories, one containing 453 malwares apks and another containing 227 non-malware apks. The malware samples are collected from \emph{Android Malware Genome Project}. The non-malwares are directly downloaded from Google Play Store. We run the script twice. Once given the directory of malwares, and again for directory of non-malwares. After the execution, we are left with 453 malware trace files and 227 non-malware trace files. A sample single trace file is shown in figure~\ref{fig:single-strace-log}.

\begin{figure}[h!]
    \centering
    \includegraphics[width=1\textwidth]{single-strace-log.png}
    \caption{System call trace of an application}
    \label{fig:single-strace-log}
\end{figure}

\section{Evaluating our model}
\label{sec:syscalleval}

We wrote a java program (Provided in Appendix B, See Listing B.1) which further processes these files and assess our model.

The program divides the trace files into two datasets, training and validation. 50 malwares and 50 non-malware traces are chosen randomly and put in the validation dataset. The rest of the traces are used to train the classifier. The details of the training and classification steps are described in the following sub-sections.

\subsection{Training}
\label{subsec:syscallevaltrain}

The program aggregates all the traces in the training dataset and produce two relation matrices $M_{mal}$ and $M_{nmal}$. $M_{mal}$ and $M_{nmal}$ are defined in the previous chapter. A sample relation matrix is shown in figure~\ref{fig:sample-matrix}.

\begin{figure}[h!]
    \centering
    \includegraphics[width=1\textwidth]{relation_matrix2.png}
    \caption{A sample relation matrix between syscalls and apps}
    \label{fig:sample-matrix}
\end{figure}

The two relation matrices are used to calculate the \textbf{Goodness rating}s of all syscalls.

\subsection{Classification}
\label{subsec:syscallevalclassify}
After \textbf{Goodness rating}s of all apps have been calculated, our model is ready to calssify an unlabeled app as malware or non-malware, given its system call trace. The same program calculates Goodness raings of all applications in the validation dataset, using the equation given in section~\ref{sec:syscalltheoryclassify}. If the Goodness rating of an app is lower than a \textbf{Threshold ($T$)}, our model/program flags the app as a malware, otherwise the app is considered to be non-malware.\\

In this chapter, we discussed the experiment for a single run. Actually the experiment was run multiple times, with different \textbf{Threshold} values. The reason behind this and the results achieved from our experiment is outlined in the following chapter.




















\chapter{Results}
\label{chap:results}
The results achieved from our experiment described in chapter~\ref{chap:syscallexperiment} is discussed in detail in this chapter.\\

In section~\ref{sec:syscalltheoryclassify}, we introduced a \textbf{Threshold} ($\tau$) and stated that the value of this threshold should be derived experimentally. It is dependent on the training dataset. Therefore, to find a reasonable value for $\tau$, we tried different thresholds and for each threshold, we ran our classifier for all validation apps and calculated different metrics like \textbf{accuracy} ($\alpha$), \textbf{recall} ($\rho$), \textbf{specificity} ($\sigma$), \textbf{precision} ($\phi$) and \textbf{F-measure} ($F$). These metrics are defined in section~\ref{sec:netanalysisresults}. We started from a threshold value of $-200$ and ended with $1500$, with step $10$. So the classifier was run a total of 171 times (each time with all validation apps).\\

Ideally, the \textbf{Threshold ($\tau$)} should be zero, but our experiment showed better accuracy for other values. To be exact, the best accuracy (87\%) is achieved when we use a threshold value between 300-340.\\

The \textbf{Threshold ($\tau$)} vs \textbf{Accuracy} ($\alpha$) graph is shown in Figure~\ref{fig:thvsacc}.\\

\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{thvsacc2.png}
    \caption{\textbf{Threshold ($\tau$)} vs \textbf{Accuracy} ($\alpha$) graph}
    \label{fig:thvsacc}
\end{figure}

The effect of threshold on other performance metrics of the classifier is shown in Figure~\ref{fig:thvsppv} to~\ref{fig:thvsfm}.\\

\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{thvsprecision.png}
    \caption{\textbf{Threshold ($\tau$)} vs \textbf{precision} ($\phi$) graph}
    \label{fig:thvsppv}
\end{figure}

With increasing threshold, the classifier would tend to classify more apps as malware. So precision falls with increasing threshold. \textbf{Threshold} ($\tau$) vs \textbf{precision} ($\phi$) graph in figure~\ref{fig:thvsppv} shows $\phi = 87.8\%$ for $\tau = 320$, which is more than acceptable.\\

\textbf{Specificity} ($\sigma$), also decreases as threshold increases. In figure~\ref{fig:thvsspc}, \textbf{Threshold} ($\tau$) vs \textbf{specificity} ($\sigma$) graph shows $\sigma = 82.7\%$ for $\tau = 320$. So $320$ is a good value for threshold considering specificity.\\

\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{thvsspc2.png}
    \caption{\textbf{Threshold ($\tau$)} vs \textbf{specificity} ($\sigma$) graph}
    \label{fig:thvsspc}
\end{figure}

Detection rate of known malwares as malwares (recall) increases with threshold. Again, the \textbf{Threshold ($\tau$)} vs \textbf{recall} ($\rho$) graph in figure~\ref{fig:thvstpr} shows an excellent value of \textbf{recall}, $\rho = 90.1\%$ for $\tau = 320$.\\

\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{thvsrecall.png}
    \caption{\textbf{Threshold ($\tau$)} vs \textbf{recall} ($\rho$) graph}
    \label{fig:thvstpr}
\end{figure}

And at last, Figure~\ref{fig:thvsfm} also shows an excellent \textbf{f-measure} of $0.889$ for $\tau = 320$.\\

\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{thvsfm2.png}
    \caption{\textbf{Threshold ($\tau$)} vs \textbf{f-measure} ($F$) graph}
    \label{fig:thvsfm}
\end{figure}

According to all these performance metrics, $320$ seems to be a very plausible value as \textbf{Threshold}, ($\tau$) for our model. The \textbf{Confusion Matrix} of our model resulted from using $\tau = 320$ is shown in Figure~\ref{fig:confmatrix}.\\

\begin{figure}
    \centering
    \includegraphics[width=0.75\textwidth]{confmatrix.png}
    \caption{\textbf{Confusion Matrix} with $\tau = 320$}
    \label{fig:confmatrix}
\end{figure}

Although we used widely variying types of malware and non-malware application samples in training and validation of our data, it is very difficult to amass a set of malware and non-malware apps that correctly emulates the distribution of all malwares in the wild and all non-malware apps in Google Play Store. So our experimentally achieved value for parameters like threshold might not be a good choice in all cases. But if we can feed the classifier a decent representative set of malwares and non-malwares, it should produce very usable results.

